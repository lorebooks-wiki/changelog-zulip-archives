<html>
<head><meta charset="utf-8"><title>672: From GitLab to Kilo Code · interviews · Zulip Chat Archive</title></head>
<h2>Stream: <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/index.html">interviews</a></h2>
<h3>Topic: <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html">672: From GitLab to Kilo Code</a></h3>

<hr>

<base href="https://changelog.zulipchat.com">

<head><link href="http://changelog.zulip-archive.lorebooks.wiki/style.css" rel="stylesheet"></head>

<a name="566822308"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/566822308" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Logbot <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#566822308">(Jan 07 2026 at 21:00)</a>:</h4>
<p>We're joined by Sid Sijbrandij, founder of GitLab who led the all-in-one coding platform all the way to IPO. In late 2022, Sid discovered that he had bone cancer. That started a journey he's been on ever since... a journey that he shares with us in great detail. Along the way, Sid continued founding companies including <a href="https://kilo.ai/">Kilo Code</a>, an all-in-one agentic engineering platform, which he also tells us all about. <span aria-label="link" class="emoji emoji-1f517" role="img" title="link">:link:</span> <a href="https://changelog.fm/672">https://changelog.fm/672</a></p>
<table>
<thead>
<tr>
<th>Ch</th>
<th>Start</th>
<th>Title</th>
<th>Runs</th>
</tr>
</thead>
<tbody>
<tr>
<td>01</td>
<td><a href="https://changelog.fm/672#t=0">00:00</a></td>
<td>Welcome to The Changelog</td>
<td>00:56</td>
</tr>
<tr>
<td>02</td>
<td><a href="https://changelog.fm/672#t=56">00:56</a></td>
<td><a href="https://depot.dev">Sponsor: Depot</a></td>
<td>02:22</td>
</tr>
<tr>
<td>03</td>
<td><a href="https://changelog.fm/672#t=199">03:19</a></td>
<td>Start the show!</td>
<td>00:55</td>
</tr>
<tr>
<td>04</td>
<td><a href="https://changelog.fm/672#t=254">04:14</a></td>
<td>Sid's story</td>
<td>02:09</td>
</tr>
<tr>
<td>05</td>
<td><a href="https://changelog.fm/672#t=383">06:23</a></td>
<td>A health crisis</td>
<td>03:29</td>
</tr>
<tr>
<td>06</td>
<td><a href="https://changelog.fm/672#t=592">09:52</a></td>
<td>Doing it in parallel</td>
<td>03:17</td>
</tr>
<tr>
<td>07</td>
<td><a href="https://changelog.fm/672#t=789">13:09</a></td>
<td>Why go to China</td>
<td>01:13</td>
</tr>
<tr>
<td>08</td>
<td><a href="https://changelog.fm/672#t=862">14:22</a></td>
<td>Sid's health today</td>
<td>00:58</td>
</tr>
<tr>
<td>09</td>
<td><a href="https://changelog.fm/672#t=920">15:20</a></td>
<td>The useful medicines</td>
<td>04:28</td>
</tr>
<tr>
<td>10</td>
<td><a href="https://changelog.fm/672#t=1188">19:48</a></td>
<td>The type of cancer</td>
<td>06:52</td>
</tr>
<tr>
<td>11</td>
<td><a href="https://changelog.fm/672#t=1600">26:40</a></td>
<td>What's next?</td>
<td>03:18</td>
</tr>
<tr>
<td>12</td>
<td><a href="https://changelog.fm/672#t=1798">29:58</a></td>
<td><a href="https://www.tigerdata.com">Sponsor: Tiger Data</a></td>
<td>02:29</td>
</tr>
<tr>
<td>13</td>
<td><a href="https://changelog.fm/672#t=1947">32:27</a></td>
<td>Working on Kilo</td>
<td>03:02</td>
</tr>
<tr>
<td>14</td>
<td><a href="https://changelog.fm/672#t=2129">35:29</a></td>
<td>Open core Kilo</td>
<td>00:41</td>
</tr>
<tr>
<td>15</td>
<td><a href="https://changelog.fm/672#t=2169">36:09</a></td>
<td>Approaching models</td>
<td>03:33</td>
</tr>
<tr>
<td>16</td>
<td><a href="https://changelog.fm/672#t=2382">39:42</a></td>
<td>The all-in-one challenge</td>
<td>03:01</td>
</tr>
<tr>
<td>17</td>
<td><a href="https://changelog.fm/672#t=2563">42:43</a></td>
<td>More parallels</td>
<td>01:59</td>
</tr>
<tr>
<td>18</td>
<td><a href="https://changelog.fm/672#t=2682">44:42</a></td>
<td>Sounds pricey</td>
<td>02:56</td>
</tr>
<tr>
<td>19</td>
<td><a href="https://changelog.fm/672#t=2858">47:38</a></td>
<td>Future budgets</td>
<td>01:34</td>
</tr>
<tr>
<td>20</td>
<td><a href="https://changelog.fm/672#t=2952">49:12</a></td>
<td>The ultimate polymath</td>
<td>07:00</td>
</tr>
<tr>
<td>21</td>
<td><a href="https://changelog.fm/672#t=3372">56:12</a></td>
<td><a href="http://notion.com/changelog">Sponsor: Notion</a></td>
<td>02:50</td>
</tr>
<tr>
<td>22</td>
<td><a href="https://changelog.fm/672#t=3542">59:02</a></td>
<td>Competing with Cursor</td>
<td>04:44</td>
</tr>
<tr>
<td>23</td>
<td><a href="https://changelog.fm/672#t=3825">1:03:45</a></td>
<td>Kilo's UX</td>
<td>01:22</td>
</tr>
<tr>
<td>24</td>
<td><a href="https://changelog.fm/672#t=3908">1:05:08</a></td>
<td>Claude Web has repos</td>
<td>03:16</td>
</tr>
<tr>
<td>25</td>
<td><a href="https://changelog.fm/672#t=4104">1:08:24</a></td>
<td>Sid the polymath?</td>
<td>00:30</td>
</tr>
<tr>
<td>26</td>
<td><a href="https://changelog.fm/672#t=4134">1:08:54</a></td>
<td>The developer's future</td>
<td>03:43</td>
</tr>
<tr>
<td>27</td>
<td><a href="https://changelog.fm/672#t=4357">1:12:37</a></td>
<td>Hiring?</td>
<td>01:57</td>
</tr>
<tr>
<td>28</td>
<td><a href="https://changelog.fm/672#t=4474">1:14:34</a></td>
<td>Wrapping up</td>
<td>01:20</td>
</tr>
<tr>
<td>29</td>
<td><a href="https://changelog.fm/672#t=4555">1:15:55</a></td>
<td>Closing thoughts</td>
<td>01:22</td>
</tr>
</tbody>
</table>



<a name="567436066"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/567436066" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Don MacKinnon <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#567436066">(Jan 12 2026 at 03:38)</a>:</h4>
<p>Glad Sid's cancer is in remission, as someone who's had a lot of family go through it I know how difficult it can be.  Sounds like he's doing a lot of amazing work with the companies that he is building and the cancer treatment initiatives he's pushing forward.  I did have one nitpick about the conversation, Sid mentioned that AGI is "already here" but if he's referring to the 2023 paper, that opinion is not generally accepted by the experts in the fields. Most are estimating AGI to arrive somewhere around 2040. I know a lot of the VC backed companies would like folks to think otherwise or want to change the definition of what AGI is but it's not the case.</p>



<a name="567454824"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/567454824" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ron Waldon-Howe <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#567454824">(Jan 12 2026 at 07:33)</a>:</h4>
<p>yeah, they've backed themselves into a corner by hyping up "AGI" all whilst pouring money into a technology that seems less and less likely to actually lead to AGI</p>
<p>not without making the term meaningless, at least</p>
<p>to me, i would want a successor to LLM technology to actually understand and process facts and instructions in some way, rather than pretending that it does this, so that hallucinations/fabrications and injection attacks are 100% eliminated</p>



<a name="567565877"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/567565877" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Don MacKinnon <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#567565877">(Jan 12 2026 at 15:26)</a>:</h4>
<p>The sentiment I have heard is that it's likely LLMs won't be the path to AGI, like you mentioned they have no understanding or reasoning. They're merely statistically predictive completions</p>



<a name="567588749"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/567588749" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Jerod Santo <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#567588749">(Jan 12 2026 at 16:47)</a>:</h4>
<p>I also took issue with the claim, hence my surprised response. I chose not to drill down after he stated the definition he was using, because:</p>
<p>A) I don’t know the reference<br>
B) I didn’t want to derail the conversation</p>



<a name="567657276"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/567657276" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ron Waldon-Howe <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#567657276">(Jan 12 2026 at 22:56)</a>:</h4>
<p>yeah, there's an art to interviews that I don't pretend to grasp &lt;3</p>



<a name="567678823"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/567678823" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Tim Uckun <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#567678823">(Jan 13 2026 at 03:32)</a>:</h4>
<p>I don't know where I heard this from but..... "Everything the LLM says is a hallucination, it's just that you only recognize some of them". The point is that LLMs don't have an internal representation of the world. They are continually hallucinating.</p>



<a name="567951456"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/567951456" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Tim Uckun <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#567951456">(Jan 14 2026 at 10:42)</a>:</h4>
<p>I got really excited after hearing this episode do I installed kilo into vs code. I don't know why or how but after struggling for a while could not get it to work properly. I got the api keys for mistral, gemini and gemini cli and put them all in, I signed up for kilo and put the API key in etc but still won't work. I think the product needs a little more polish to make onboarding easier.</p>



<a name="569786948"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/569786948" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> James McNally <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#569786948">(Jan 23 2026 at 19:08)</a>:</h4>
<p>Gave Kilo a go after this as I wanted to try a set an agent running in the background directly to github. Set it on a reasonable alteration to an open source library.</p>
<p>It's been my first experience of API pricing for this an s quickly burnt through $30. With estimates the real price is many times this it was interesting to see the real cost on a problem.</p>
<p>My estimate is this would have taken me about 4 hours and probably spent at least 0.5-1 hours babysitting, pointing out issues and cleaning up after. It's actually still not working, needs one last clean up.</p>
<p>It's interesting I'm hearing a lot more positivity around this use case and will be trialling more over the coming month but I'll probably stick to my Claude subscription for now!</p>



<a name="569787052"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/569787052" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> James McNally <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#569787052">(Jan 23 2026 at 19:09)</a>:</h4>
<p>I am excited about having a central platform to work across gitlab and GitHub though - I like the philosophy behind itz certainly some rough edges still through</p>



<a name="569850861"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/569850861" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Tim Uckun <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#569850861">(Jan 24 2026 at 09:12)</a>:</h4>
<p>I set up lmstudio and ollama on the mac. I then installed <a href="http://continue.dev">continue.dev</a> and then after a bit of fiddling connected it to lmstudio to use with my locally installed llms. I did run into a problem and apparently I ran out of context so I fiddled with my settings but I haven't tried it again to see what happens if I push it a bit. The performance of the local LLM seemed fine, a little slower than gemini on my machine but not much slower.</p>



<a name="569851966"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/569851966" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ron Waldon-Howe <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#569851966">(Jan 24 2026 at 09:34)</a>:</h4>
<p><span class="user-mention silent" data-user-id="755905">Tim Uckun</span> <a href="#narrow/channel/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code/near/569850861">said</a>:</p>
<blockquote>
<p>The performance of the local LLM seemed fine, a little slower than gemini on my machine but not much slower.</p>
</blockquote>
<p>oh, which model(s) are you using locally? how many parameters?</p>



<a name="569902598"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/569902598" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Tim Uckun <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#569902598">(Jan 24 2026 at 20:47)</a>:</h4>
<p>I am using the gpt30b and qwen 2.5b for completions both in mlx formats. the 30B models are at the limit of what my machine can do though so I am going to downgrade to a 20  (or less) tonight so I have some spare ram.  I am still learning all this stuff though and there are a lot of buttons to push to make things better or worse. Quantization, temperature etc.  Most of the MLX models are provided by the community too so it's always a bit of a mystery what you are getting and how it's going to work. </p>
<p>BTW AFIK I know ollama doesn't support MLX out of the box yet which is why I am using LMStudio.</p>



<a name="569908943"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/569908943" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ron Waldon-Howe <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#569908943">(Jan 24 2026 at 22:55)</a>:</h4>
<p>I'm guessing <a href="http://continue.dev">continue.dev</a> is an alternative to <a href="https://block.github.io/goose/">https://block.github.io/goose/</a> ? sort of an orchestrator for agents/sessions?</p>



<a name="569909024"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/569909024" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ron Waldon-Howe <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#569909024">(Jan 24 2026 at 22:56)</a>:</h4>
<p>yeah, i've heard that folks are getting decent results with newer 3B parameter models these days, but i've still not had anything basic work<br>
keen to figure it out though<br>
it also doesn't help that i haven't been able to get ollama to use Vulkan nor ROCm, so it's all on my CPU<br>
I should look into ollama-alternatives like you have</p>



<a name="569921906"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/569921906" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Tim Uckun <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#569921906">(Jan 25 2026 at 03:28)</a>:</h4>
<p>I haven't given ollama a serious go but I will for sure because they are continually working on it and it is the "standard".  I also plan on using smaller models with bigger context but I admit I still have a lot to learn about the nuances of these models and the all the parameters I can tune when running them.</p>



<a name="570004631"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/570004631" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Siddhartha Golu <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#570004631">(Jan 26 2026 at 03:29)</a>:</h4>
<p>The consensus in the community, or at least at /r/LocalLLAMA, is that Ollama has really gone down the drain with recent updates (or lack thereof). It was always a pretty wrapper on llama.cpp and now that llama.cpp has a built-in web UI and a way to hot switch b/w different models, it makes sense to set the default to llama.cpp. Everything else is using llama.cpp as its base anyway.</p>
<p>I use a local autocompletion model in neovim, using <a href="https://github.com/ggml-org/llama.vim">llama.vim</a> and llama.cpp, and it works surprisingly well! Using <code>qwen2.5-coder-7b</code> along with <code>qwen2.5-coder-0.5b</code> as a draft model for speculative decoding.</p>



<a name="570013773"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/570013773" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ron Waldon-Howe <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#570013773">(Jan 26 2026 at 05:42)</a>:</h4>
<p>yeah, i was using lsp-ai (EOL?) to supply LLM auto completions from ollama into helix, and that was working quite well<br>
another constraint i have is that i want to use the same setup between personal and work, so i have to avoid anything with commercial limitations in its licence (although I think Qwen is Apache?)</p>



<a name="570415044"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/570415044" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Tim Uckun <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#570415044">(Jan 27 2026 at 20:22)</a>:</h4>
<p>I found out that 24GB is just not enough to run a decent sized model. It runs the 30B models but there isn't enough context to make them useful. I tried the 14b models and they do have enough context but on my machine it's just too slow to be useful. I am still going to try to run small qwen coders as a code complete though but am going to revert to gemini for planning and larger tasks. It may not be the best but it's good enough and I don't use it enough to go above the free tier.</p>



<a name="571271102"></a>
<h4><a href="https://changelog.zulipchat.com#narrow/stream/456187-interviews/topic/672%3A%20From%20GitLab%20to%20Kilo%20Code/near/571271102" class="zl"><img src="http://changelog.zulip-archive.lorebooks.wiki/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ron Waldon-Howe <a href="http://changelog.zulip-archive.lorebooks.wiki/stream/456187-interviews/topic/672.3A.20From.20GitLab.20to.20Kilo.20Code.html#571271102">(Feb 01 2026 at 09:21)</a>:</h4>
<p>wow, i've just had something actually work 100% locally for the first time<br>
it took 43 minutes due to difficulty getting ollama to use my AMD GPU, but used goose+ollama+ministral-3:3b to use <code>ls</code> and <code>cat</code> to look at my dotfiles project and return a not-completely-wrong summary of it<br>
hilariously, it must have used my <code>$SHELL</code> which is nushell, because I saw the tabular <code>ls</code> output along the way</p>



<hr><p>Last updated: Feb 17 2026 at 17:33 UTC</p>
</html>